{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dfaf8f1-33d4-4fb4-9704-dd9fe6c6c23d",
   "metadata": {},
   "source": [
    "https://github.com/seungeunrho/minimalRL\n",
    "위의 깃허브 링크를 참조해서 Policy Gradient 중에 Reinforce를 구현한 코드\n",
    "Reinforce : 정책을 확률로 표현하고, 보상 크기에 따라 그 확률을 그대로 미분해 업데이트 하는 정책 경사법의 원형 형태이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9877dc9-505a-4e9a-8774-7ce2d46bb0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91028825-ea5b-4b07-979b-5e4e2ac31bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of episode: 20, Avg timestpe : 21.6\n",
      "# of episode: 40, Avg timestpe : 24.1\n",
      "# of episode: 60, Avg timestpe : 28.3\n",
      "# of episode: 80, Avg timestpe : 19.2\n",
      "# of episode: 100, Avg timestpe : 24.7\n",
      "# of episode: 120, Avg timestpe : 28.95\n",
      "# of episode: 140, Avg timestpe : 28.3\n",
      "# of episode: 160, Avg timestpe : 23.9\n",
      "# of episode: 180, Avg timestpe : 28.0\n",
      "# of episode: 200, Avg timestpe : 36.05\n",
      "# of episode: 220, Avg timestpe : 24.1\n",
      "# of episode: 240, Avg timestpe : 26.95\n",
      "# of episode: 260, Avg timestpe : 30.7\n",
      "# of episode: 280, Avg timestpe : 30.6\n",
      "# of episode: 300, Avg timestpe : 22.35\n",
      "# of episode: 320, Avg timestpe : 32.7\n",
      "# of episode: 340, Avg timestpe : 23.8\n",
      "# of episode: 360, Avg timestpe : 38.95\n",
      "# of episode: 380, Avg timestpe : 44.6\n",
      "# of episode: 400, Avg timestpe : 30.45\n",
      "# of episode: 420, Avg timestpe : 47.5\n",
      "# of episode: 440, Avg timestpe : 35.15\n",
      "# of episode: 460, Avg timestpe : 42.35\n",
      "# of episode: 480, Avg timestpe : 48.85\n",
      "# of episode: 500, Avg timestpe : 40.0\n",
      "# of episode: 520, Avg timestpe : 52.45\n",
      "# of episode: 540, Avg timestpe : 48.05\n",
      "# of episode: 560, Avg timestpe : 48.6\n",
      "# of episode: 580, Avg timestpe : 66.2\n",
      "# of episode: 600, Avg timestpe : 68.05\n",
      "# of episode: 620, Avg timestpe : 61.25\n",
      "# of episode: 640, Avg timestpe : 72.85\n",
      "# of episode: 660, Avg timestpe : 50.55\n",
      "# of episode: 680, Avg timestpe : 65.8\n",
      "# of episode: 700, Avg timestpe : 74.8\n",
      "# of episode: 720, Avg timestpe : 71.25\n",
      "# of episode: 740, Avg timestpe : 53.3\n",
      "# of episode: 760, Avg timestpe : 68.65\n",
      "# of episode: 780, Avg timestpe : 64.65\n",
      "# of episode: 800, Avg timestpe : 87.8\n",
      "# of episode: 820, Avg timestpe : 92.75\n",
      "# of episode: 840, Avg timestpe : 92.9\n",
      "# of episode: 860, Avg timestpe : 117.7\n",
      "# of episode: 880, Avg timestpe : 105.0\n",
      "# of episode: 900, Avg timestpe : 111.35\n",
      "# of episode: 920, Avg timestpe : 92.2\n",
      "# of episode: 940, Avg timestpe : 106.8\n",
      "# of episode: 960, Avg timestpe : 118.7\n",
      "# of episode: 980, Avg timestpe : 110.35\n",
      "# of episode: 1000, Avg timestpe : 117.6\n",
      "# of episode: 1020, Avg timestpe : 128.2\n",
      "# of episode: 1040, Avg timestpe : 115.95\n",
      "# of episode: 1060, Avg timestpe : 130.2\n",
      "# of episode: 1080, Avg timestpe : 151.15\n",
      "# of episode: 1100, Avg timestpe : 148.55\n",
      "# of episode: 1120, Avg timestpe : 175.65\n",
      "# of episode: 1140, Avg timestpe : 152.95\n",
      "# of episode: 1160, Avg timestpe : 133.0\n",
      "# of episode: 1180, Avg timestpe : 153.2\n",
      "# of episode: 1200, Avg timestpe : 160.5\n",
      "# of episode: 1220, Avg timestpe : 165.0\n",
      "# of episode: 1240, Avg timestpe : 162.25\n",
      "# of episode: 1260, Avg timestpe : 165.6\n",
      "# of episode: 1280, Avg timestpe : 180.2\n",
      "# of episode: 1300, Avg timestpe : 160.95\n",
      "# of episode: 1320, Avg timestpe : 175.8\n",
      "# of episode: 1340, Avg timestpe : 180.55\n",
      "# of episode: 1360, Avg timestpe : 182.45\n",
      "# of episode: 1380, Avg timestpe : 217.2\n",
      "# of episode: 1400, Avg timestpe : 218.95\n",
      "# of episode: 1420, Avg timestpe : 236.9\n",
      "# of episode: 1440, Avg timestpe : 223.5\n",
      "# of episode: 1460, Avg timestpe : 194.15\n",
      "# of episode: 1480, Avg timestpe : 238.55\n",
      "# of episode: 1500, Avg timestpe : 223.35\n",
      "# of episode: 1520, Avg timestpe : 197.05\n",
      "# of episode: 1540, Avg timestpe : 233.8\n",
      "# of episode: 1560, Avg timestpe : 269.05\n",
      "# of episode: 1580, Avg timestpe : 245.55\n",
      "# of episode: 1600, Avg timestpe : 213.0\n",
      "# of episode: 1620, Avg timestpe : 233.05\n",
      "# of episode: 1640, Avg timestpe : 197.65\n",
      "# of episode: 1660, Avg timestpe : 246.8\n",
      "# of episode: 1680, Avg timestpe : 206.25\n",
      "# of episode: 1700, Avg timestpe : 226.3\n",
      "# of episode: 1720, Avg timestpe : 255.35\n",
      "# of episode: 1740, Avg timestpe : 264.05\n",
      "# of episode: 1760, Avg timestpe : 242.45\n",
      "# of episode: 1780, Avg timestpe : 264.75\n",
      "# of episode: 1800, Avg timestpe : 240.3\n",
      "# of episode: 1820, Avg timestpe : 260.0\n",
      "# of episode: 1840, Avg timestpe : 259.8\n",
      "# of episode: 1860, Avg timestpe : 272.25\n",
      "# of episode: 1880, Avg timestpe : 224.9\n",
      "# of episode: 1900, Avg timestpe : 282.85\n",
      "# of episode: 1920, Avg timestpe : 260.4\n",
      "# of episode: 1940, Avg timestpe : 301.05\n",
      "# of episode: 1960, Avg timestpe : 310.15\n",
      "# of episode: 1980, Avg timestpe : 347.2\n",
      "# of episode: 2000, Avg timestpe : 290.55\n",
      "# of episode: 2020, Avg timestpe : 305.95\n",
      "# of episode: 2040, Avg timestpe : 301.9\n",
      "# of episode: 2060, Avg timestpe : 336.95\n",
      "# of episode: 2080, Avg timestpe : 344.4\n",
      "# of episode: 2100, Avg timestpe : 325.1\n",
      "# of episode: 2120, Avg timestpe : 229.15\n",
      "# of episode: 2140, Avg timestpe : 266.95\n",
      "# of episode: 2160, Avg timestpe : 246.0\n",
      "# of episode: 2180, Avg timestpe : 342.7\n",
      "# of episode: 2200, Avg timestpe : 313.2\n",
      "# of episode: 2220, Avg timestpe : 286.5\n",
      "# of episode: 2240, Avg timestpe : 336.55\n",
      "# of episode: 2260, Avg timestpe : 286.55\n",
      "# of episode: 2280, Avg timestpe : 311.45\n",
      "# of episode: 2300, Avg timestpe : 275.5\n",
      "# of episode: 2320, Avg timestpe : 382.8\n",
      "# of episode: 2340, Avg timestpe : 358.45\n",
      "# of episode: 2360, Avg timestpe : 387.15\n",
      "# of episode: 2380, Avg timestpe : 328.75\n",
      "# of episode: 2400, Avg timestpe : 318.9\n",
      "# of episode: 2420, Avg timestpe : 316.5\n",
      "# of episode: 2440, Avg timestpe : 302.9\n",
      "# of episode: 2460, Avg timestpe : 330.25\n",
      "# of episode: 2480, Avg timestpe : 329.35\n",
      "# of episode: 2500, Avg timestpe : 309.7\n",
      "# of episode: 2520, Avg timestpe : 275.1\n",
      "# of episode: 2540, Avg timestpe : 343.65\n",
      "# of episode: 2560, Avg timestpe : 339.45\n",
      "# of episode: 2580, Avg timestpe : 349.55\n",
      "# of episode: 2600, Avg timestpe : 349.95\n",
      "# of episode: 2620, Avg timestpe : 398.4\n",
      "# of episode: 2640, Avg timestpe : 384.0\n",
      "# of episode: 2660, Avg timestpe : 335.1\n",
      "# of episode: 2680, Avg timestpe : 294.75\n",
      "# of episode: 2700, Avg timestpe : 315.6\n",
      "# of episode: 2720, Avg timestpe : 307.7\n",
      "# of episode: 2740, Avg timestpe : 329.4\n",
      "# of episode: 2760, Avg timestpe : 365.1\n",
      "# of episode: 2780, Avg timestpe : 415.9\n",
      "# of episode: 2800, Avg timestpe : 354.7\n",
      "# of episode: 2820, Avg timestpe : 360.75\n",
      "# of episode: 2840, Avg timestpe : 374.75\n",
      "# of episode: 2860, Avg timestpe : 366.25\n",
      "# of episode: 2880, Avg timestpe : 382.7\n",
      "# of episode: 2900, Avg timestpe : 428.55\n",
      "# of episode: 2920, Avg timestpe : 369.9\n",
      "# of episode: 2940, Avg timestpe : 373.35\n",
      "# of episode: 2960, Avg timestpe : 369.7\n",
      "# of episode: 2980, Avg timestpe : 397.45\n",
      "# of episode: 3000, Avg timestpe : 368.1\n",
      "# of episode: 3020, Avg timestpe : 415.0\n",
      "# of episode: 3040, Avg timestpe : 363.85\n",
      "# of episode: 3060, Avg timestpe : 438.85\n",
      "# of episode: 3080, Avg timestpe : 383.7\n",
      "# of episode: 3100, Avg timestpe : 427.2\n",
      "# of episode: 3120, Avg timestpe : 421.05\n",
      "# of episode: 3140, Avg timestpe : 437.75\n",
      "# of episode: 3160, Avg timestpe : 390.65\n",
      "# of episode: 3180, Avg timestpe : 404.3\n",
      "# of episode: 3200, Avg timestpe : 339.8\n",
      "# of episode: 3220, Avg timestpe : 358.6\n",
      "# of episode: 3240, Avg timestpe : 418.35\n",
      "# of episode: 3260, Avg timestpe : 413.85\n",
      "# of episode: 3280, Avg timestpe : 446.15\n",
      "# of episode: 3300, Avg timestpe : 445.65\n",
      "# of episode: 3320, Avg timestpe : 400.3\n",
      "# of episode: 3340, Avg timestpe : 437.25\n",
      "# of episode: 3360, Avg timestpe : 409.5\n",
      "# of episode: 3380, Avg timestpe : 398.25\n",
      "# of episode: 3400, Avg timestpe : 415.55\n",
      "# of episode: 3420, Avg timestpe : 439.0\n",
      "# of episode: 3440, Avg timestpe : 414.45\n",
      "# of episode: 3460, Avg timestpe : 396.6\n",
      "# of episode: 3480, Avg timestpe : 409.55\n",
      "# of episode: 3500, Avg timestpe : 365.25\n",
      "# of episode: 3520, Avg timestpe : 416.7\n",
      "# of episode: 3540, Avg timestpe : 417.55\n",
      "# of episode: 3560, Avg timestpe : 439.85\n",
      "# of episode: 3580, Avg timestpe : 462.3\n",
      "# of episode: 3600, Avg timestpe : 441.65\n",
      "# of episode: 3620, Avg timestpe : 440.4\n",
      "# of episode: 3640, Avg timestpe : 351.3\n",
      "# of episode: 3660, Avg timestpe : 407.7\n",
      "# of episode: 3680, Avg timestpe : 389.05\n",
      "# of episode: 3700, Avg timestpe : 421.25\n",
      "# of episode: 3720, Avg timestpe : 458.35\n",
      "# of episode: 3740, Avg timestpe : 400.4\n",
      "# of episode: 3760, Avg timestpe : 404.9\n",
      "# of episode: 3780, Avg timestpe : 392.0\n",
      "# of episode: 3800, Avg timestpe : 400.1\n",
      "# of episode: 3820, Avg timestpe : 296.45\n",
      "# of episode: 3840, Avg timestpe : 369.25\n",
      "# of episode: 3860, Avg timestpe : 335.0\n",
      "# of episode: 3880, Avg timestpe : 307.5\n",
      "# of episode: 3900, Avg timestpe : 377.1\n",
      "# of episode: 3920, Avg timestpe : 390.55\n",
      "# of episode: 3940, Avg timestpe : 383.3\n",
      "# of episode: 3960, Avg timestpe : 430.15\n",
      "# of episode: 3980, Avg timestpe : 419.1\n",
      "# of episode: 4000, Avg timestpe : 415.6\n",
      "# of episode: 4020, Avg timestpe : 444.75\n",
      "# of episode: 4040, Avg timestpe : 411.5\n",
      "# of episode: 4060, Avg timestpe : 397.9\n",
      "# of episode: 4080, Avg timestpe : 380.85\n",
      "# of episode: 4100, Avg timestpe : 458.2\n",
      "# of episode: 4120, Avg timestpe : 407.55\n",
      "# of episode: 4140, Avg timestpe : 441.35\n",
      "# of episode: 4160, Avg timestpe : 412.6\n",
      "# of episode: 4180, Avg timestpe : 467.8\n",
      "# of episode: 4200, Avg timestpe : 447.9\n",
      "# of episode: 4220, Avg timestpe : 430.35\n",
      "# of episode: 4240, Avg timestpe : 456.05\n",
      "# of episode: 4260, Avg timestpe : 452.55\n",
      "# of episode: 4280, Avg timestpe : 425.65\n",
      "# of episode: 4300, Avg timestpe : 429.55\n",
      "# of episode: 4320, Avg timestpe : 470.25\n",
      "# of episode: 4340, Avg timestpe : 461.2\n",
      "# of episode: 4360, Avg timestpe : 438.15\n",
      "# of episode: 4380, Avg timestpe : 424.85\n",
      "# of episode: 4400, Avg timestpe : 457.35\n",
      "# of episode: 4420, Avg timestpe : 442.1\n",
      "# of episode: 4440, Avg timestpe : 449.75\n",
      "# of episode: 4460, Avg timestpe : 432.2\n",
      "# of episode: 4480, Avg timestpe : 453.65\n",
      "# of episode: 4500, Avg timestpe : 429.55\n",
      "# of episode: 4520, Avg timestpe : 489.35\n",
      "# of episode: 4540, Avg timestpe : 444.1\n",
      "# of episode: 4560, Avg timestpe : 422.75\n",
      "# of episode: 4580, Avg timestpe : 427.3\n",
      "# of episode: 4600, Avg timestpe : 473.45\n",
      "# of episode: 4620, Avg timestpe : 429.8\n",
      "# of episode: 4640, Avg timestpe : 436.65\n",
      "# of episode: 4660, Avg timestpe : 477.15\n",
      "# of episode: 4680, Avg timestpe : 406.2\n",
      "# of episode: 4700, Avg timestpe : 422.2\n",
      "# of episode: 4720, Avg timestpe : 428.0\n",
      "# of episode: 4740, Avg timestpe : 353.05\n",
      "# of episode: 4760, Avg timestpe : 305.15\n",
      "# of episode: 4780, Avg timestpe : 289.5\n",
      "# of episode: 4800, Avg timestpe : 266.45\n",
      "# of episode: 4820, Avg timestpe : 329.5\n",
      "# of episode: 4840, Avg timestpe : 421.8\n",
      "# of episode: 4860, Avg timestpe : 329.85\n",
      "# of episode: 4880, Avg timestpe : 298.15\n",
      "# of episode: 4900, Avg timestpe : 295.0\n",
      "# of episode: 4920, Avg timestpe : 418.0\n",
      "# of episode: 4940, Avg timestpe : 416.75\n",
      "# of episode: 4960, Avg timestpe : 447.65\n",
      "# of episode: 4980, Avg timestpe : 370.45\n",
      "# of episode: 5000, Avg timestpe : 392.55\n",
      "# of episode: 5020, Avg timestpe : 453.45\n",
      "# of episode: 5040, Avg timestpe : 435.05\n",
      "# of episode: 5060, Avg timestpe : 420.8\n",
      "# of episode: 5080, Avg timestpe : 431.25\n",
      "# of episode: 5100, Avg timestpe : 437.15\n",
      "# of episode: 5120, Avg timestpe : 397.35\n",
      "# of episode: 5140, Avg timestpe : 429.9\n",
      "# of episode: 5160, Avg timestpe : 383.0\n",
      "# of episode: 5180, Avg timestpe : 382.65\n",
      "# of episode: 5200, Avg timestpe : 399.5\n",
      "# of episode: 5220, Avg timestpe : 391.0\n",
      "# of episode: 5240, Avg timestpe : 417.15\n",
      "# of episode: 5260, Avg timestpe : 439.55\n",
      "# of episode: 5280, Avg timestpe : 395.55\n",
      "# of episode: 5300, Avg timestpe : 431.2\n",
      "# of episode: 5320, Avg timestpe : 373.15\n",
      "# of episode: 5340, Avg timestpe : 447.65\n",
      "# of episode: 5360, Avg timestpe : 399.45\n",
      "# of episode: 5380, Avg timestpe : 455.3\n",
      "# of episode: 5400, Avg timestpe : 450.0\n",
      "# of episode: 5420, Avg timestpe : 381.35\n",
      "# of episode: 5440, Avg timestpe : 312.05\n",
      "# of episode: 5460, Avg timestpe : 337.6\n",
      "# of episode: 5480, Avg timestpe : 290.85\n",
      "# of episode: 5500, Avg timestpe : 427.65\n",
      "# of episode: 5520, Avg timestpe : 430.65\n",
      "# of episode: 5540, Avg timestpe : 423.1\n",
      "# of episode: 5560, Avg timestpe : 394.65\n",
      "# of episode: 5580, Avg timestpe : 429.55\n",
      "# of episode: 5600, Avg timestpe : 409.7\n",
      "# of episode: 5620, Avg timestpe : 419.55\n",
      "# of episode: 5640, Avg timestpe : 472.15\n",
      "# of episode: 5660, Avg timestpe : 435.15\n",
      "# of episode: 5680, Avg timestpe : 436.8\n",
      "# of episode: 5700, Avg timestpe : 452.75\n",
      "# of episode: 5720, Avg timestpe : 449.4\n",
      "# of episode: 5740, Avg timestpe : 480.45\n",
      "# of episode: 5760, Avg timestpe : 477.0\n",
      "# of episode: 5780, Avg timestpe : 442.95\n",
      "# of episode: 5800, Avg timestpe : 478.15\n",
      "# of episode: 5820, Avg timestpe : 454.75\n",
      "# of episode: 5840, Avg timestpe : 461.25\n",
      "# of episode: 5860, Avg timestpe : 443.0\n",
      "# of episode: 5880, Avg timestpe : 456.7\n",
      "# of episode: 5900, Avg timestpe : 441.1\n",
      "# of episode: 5920, Avg timestpe : 477.05\n",
      "# of episode: 5940, Avg timestpe : 467.05\n",
      "# of episode: 5960, Avg timestpe : 455.05\n",
      "# of episode: 5980, Avg timestpe : 472.65\n",
      "# of episode: 6000, Avg timestpe : 471.8\n",
      "# of episode: 6020, Avg timestpe : 449.2\n",
      "# of episode: 6040, Avg timestpe : 465.0\n",
      "# of episode: 6060, Avg timestpe : 435.85\n",
      "# of episode: 6080, Avg timestpe : 428.35\n",
      "# of episode: 6100, Avg timestpe : 433.15\n",
      "# of episode: 6120, Avg timestpe : 450.55\n",
      "# of episode: 6140, Avg timestpe : 436.15\n",
      "# of episode: 6160, Avg timestpe : 451.8\n",
      "# of episode: 6180, Avg timestpe : 474.45\n",
      "# of episode: 6200, Avg timestpe : 492.45\n",
      "# of episode: 6220, Avg timestpe : 487.6\n",
      "# of episode: 6240, Avg timestpe : 500.0\n",
      "# of episode: 6260, Avg timestpe : 489.9\n",
      "# of episode: 6280, Avg timestpe : 481.85\n",
      "# of episode: 6300, Avg timestpe : 458.3\n",
      "# of episode: 6320, Avg timestpe : 485.3\n",
      "# of episode: 6340, Avg timestpe : 471.2\n",
      "# of episode: 6360, Avg timestpe : 457.45\n",
      "# of episode: 6380, Avg timestpe : 458.8\n",
      "# of episode: 6400, Avg timestpe : 442.85\n",
      "# of episode: 6420, Avg timestpe : 476.55\n",
      "# of episode: 6440, Avg timestpe : 459.35\n",
      "# of episode: 6460, Avg timestpe : 446.45\n",
      "# of episode: 6480, Avg timestpe : 475.2\n",
      "# of episode: 6500, Avg timestpe : 448.45\n",
      "# of episode: 6520, Avg timestpe : 480.65\n",
      "# of episode: 6540, Avg timestpe : 471.8\n",
      "# of episode: 6560, Avg timestpe : 478.9\n",
      "# of episode: 6580, Avg timestpe : 448.35\n",
      "# of episode: 6600, Avg timestpe : 454.55\n",
      "# of episode: 6620, Avg timestpe : 454.65\n",
      "# of episode: 6640, Avg timestpe : 462.2\n",
      "# of episode: 6660, Avg timestpe : 434.05\n",
      "# of episode: 6680, Avg timestpe : 470.2\n",
      "# of episode: 6700, Avg timestpe : 467.65\n",
      "# of episode: 6720, Avg timestpe : 467.85\n",
      "# of episode: 6740, Avg timestpe : 475.45\n",
      "# of episode: 6760, Avg timestpe : 461.4\n",
      "# of episode: 6780, Avg timestpe : 477.35\n",
      "# of episode: 6800, Avg timestpe : 433.55\n",
      "# of episode: 6820, Avg timestpe : 445.4\n",
      "# of episode: 6840, Avg timestpe : 421.3\n",
      "# of episode: 6860, Avg timestpe : 464.35\n",
      "# of episode: 6880, Avg timestpe : 449.2\n",
      "# of episode: 6900, Avg timestpe : 443.9\n",
      "# of episode: 6920, Avg timestpe : 473.7\n",
      "# of episode: 6940, Avg timestpe : 485.85\n",
      "# of episode: 6960, Avg timestpe : 465.0\n",
      "# of episode: 6980, Avg timestpe : 461.7\n",
      "# of episode: 7000, Avg timestpe : 474.0\n",
      "# of episode: 7020, Avg timestpe : 494.0\n",
      "# of episode: 7040, Avg timestpe : 467.65\n",
      "# of episode: 7060, Avg timestpe : 495.05\n",
      "# of episode: 7080, Avg timestpe : 490.45\n",
      "# of episode: 7100, Avg timestpe : 494.35\n",
      "# of episode: 7120, Avg timestpe : 473.4\n",
      "# of episode: 7140, Avg timestpe : 484.55\n",
      "# of episode: 7160, Avg timestpe : 451.1\n",
      "# of episode: 7180, Avg timestpe : 485.05\n",
      "# of episode: 7200, Avg timestpe : 476.85\n",
      "# of episode: 7220, Avg timestpe : 470.9\n",
      "# of episode: 7240, Avg timestpe : 471.7\n",
      "# of episode: 7260, Avg timestpe : 485.5\n",
      "# of episode: 7280, Avg timestpe : 497.85\n",
      "# of episode: 7300, Avg timestpe : 459.05\n",
      "# of episode: 7320, Avg timestpe : 463.85\n",
      "# of episode: 7340, Avg timestpe : 482.4\n",
      "# of episode: 7360, Avg timestpe : 479.25\n",
      "# of episode: 7380, Avg timestpe : 457.2\n",
      "# of episode: 7400, Avg timestpe : 465.15\n",
      "# of episode: 7420, Avg timestpe : 487.55\n",
      "# of episode: 7440, Avg timestpe : 497.6\n",
      "# of episode: 7460, Avg timestpe : 485.8\n",
      "# of episode: 7480, Avg timestpe : 482.4\n",
      "# of episode: 7500, Avg timestpe : 500.0\n",
      "# of episode: 7520, Avg timestpe : 493.1\n",
      "# of episode: 7540, Avg timestpe : 498.0\n",
      "# of episode: 7560, Avg timestpe : 483.45\n",
      "# of episode: 7580, Avg timestpe : 486.0\n",
      "# of episode: 7600, Avg timestpe : 424.5\n",
      "# of episode: 7620, Avg timestpe : 484.05\n",
      "# of episode: 7640, Avg timestpe : 484.05\n",
      "# of episode: 7660, Avg timestpe : 466.75\n",
      "# of episode: 7680, Avg timestpe : 457.05\n",
      "# of episode: 7700, Avg timestpe : 487.05\n",
      "# of episode: 7720, Avg timestpe : 491.65\n",
      "# of episode: 7740, Avg timestpe : 453.2\n",
      "# of episode: 7760, Avg timestpe : 487.05\n",
      "# of episode: 7780, Avg timestpe : 495.5\n",
      "# of episode: 7800, Avg timestpe : 468.05\n",
      "# of episode: 7820, Avg timestpe : 486.2\n",
      "# of episode: 7840, Avg timestpe : 488.15\n",
      "# of episode: 7860, Avg timestpe : 489.75\n",
      "# of episode: 7880, Avg timestpe : 483.0\n",
      "# of episode: 7900, Avg timestpe : 478.75\n",
      "# of episode: 7920, Avg timestpe : 458.85\n",
      "# of episode: 7940, Avg timestpe : 441.85\n",
      "# of episode: 7960, Avg timestpe : 487.35\n",
      "# of episode: 7980, Avg timestpe : 483.95\n",
      "# of episode: 8000, Avg timestpe : 481.9\n",
      "# of episode: 8020, Avg timestpe : 488.55\n",
      "# of episode: 8040, Avg timestpe : 500.0\n",
      "# of episode: 8060, Avg timestpe : 488.45\n",
      "# of episode: 8080, Avg timestpe : 493.4\n",
      "# of episode: 8100, Avg timestpe : 477.85\n",
      "# of episode: 8120, Avg timestpe : 465.35\n",
      "# of episode: 8140, Avg timestpe : 460.05\n",
      "# of episode: 8160, Avg timestpe : 425.65\n",
      "# of episode: 8180, Avg timestpe : 447.8\n",
      "# of episode: 8200, Avg timestpe : 466.95\n",
      "# of episode: 8220, Avg timestpe : 499.35\n",
      "# of episode: 8240, Avg timestpe : 493.0\n",
      "# of episode: 8260, Avg timestpe : 434.4\n",
      "# of episode: 8280, Avg timestpe : 425.25\n",
      "# of episode: 8300, Avg timestpe : 431.6\n",
      "# of episode: 8320, Avg timestpe : 421.1\n",
      "# of episode: 8340, Avg timestpe : 436.55\n",
      "# of episode: 8360, Avg timestpe : 446.45\n",
      "# of episode: 8380, Avg timestpe : 462.25\n",
      "# of episode: 8400, Avg timestpe : 441.25\n",
      "# of episode: 8420, Avg timestpe : 480.7\n",
      "# of episode: 8440, Avg timestpe : 480.35\n",
      "# of episode: 8460, Avg timestpe : 477.2\n",
      "# of episode: 8480, Avg timestpe : 491.4\n",
      "# of episode: 8500, Avg timestpe : 473.95\n",
      "# of episode: 8520, Avg timestpe : 500.0\n",
      "# of episode: 8540, Avg timestpe : 478.2\n",
      "# of episode: 8560, Avg timestpe : 486.2\n",
      "# of episode: 8580, Avg timestpe : 484.35\n",
      "# of episode: 8600, Avg timestpe : 490.35\n",
      "# of episode: 8620, Avg timestpe : 497.25\n",
      "# of episode: 8640, Avg timestpe : 482.8\n",
      "# of episode: 8660, Avg timestpe : 479.35\n",
      "# of episode: 8680, Avg timestpe : 472.85\n",
      "# of episode: 8700, Avg timestpe : 492.95\n",
      "# of episode: 8720, Avg timestpe : 493.1\n",
      "# of episode: 8740, Avg timestpe : 483.95\n",
      "# of episode: 8760, Avg timestpe : 437.2\n",
      "# of episode: 8780, Avg timestpe : 475.9\n",
      "# of episode: 8800, Avg timestpe : 492.95\n",
      "# of episode: 8820, Avg timestpe : 482.0\n",
      "# of episode: 8840, Avg timestpe : 463.45\n",
      "# of episode: 8860, Avg timestpe : 467.3\n",
      "# of episode: 8880, Avg timestpe : 461.85\n",
      "# of episode: 8900, Avg timestpe : 468.15\n",
      "# of episode: 8920, Avg timestpe : 437.9\n",
      "# of episode: 8940, Avg timestpe : 419.3\n",
      "# of episode: 8960, Avg timestpe : 459.85\n",
      "# of episode: 8980, Avg timestpe : 475.85\n",
      "# of episode: 9000, Avg timestpe : 468.7\n",
      "# of episode: 9020, Avg timestpe : 488.45\n",
      "# of episode: 9040, Avg timestpe : 456.2\n",
      "# of episode: 9060, Avg timestpe : 456.85\n",
      "# of episode: 9080, Avg timestpe : 464.0\n",
      "# of episode: 9100, Avg timestpe : 457.65\n",
      "# of episode: 9120, Avg timestpe : 492.4\n",
      "# of episode: 9140, Avg timestpe : 490.75\n",
      "# of episode: 9160, Avg timestpe : 496.0\n",
      "# of episode: 9180, Avg timestpe : 488.3\n",
      "# of episode: 9200, Avg timestpe : 492.95\n",
      "# of episode: 9220, Avg timestpe : 460.6\n",
      "# of episode: 9240, Avg timestpe : 458.15\n",
      "# of episode: 9260, Avg timestpe : 479.0\n",
      "# of episode: 9280, Avg timestpe : 449.25\n",
      "# of episode: 9300, Avg timestpe : 442.65\n",
      "# of episode: 9320, Avg timestpe : 465.3\n",
      "# of episode: 9340, Avg timestpe : 486.5\n",
      "# of episode: 9360, Avg timestpe : 490.9\n",
      "# of episode: 9380, Avg timestpe : 485.8\n",
      "# of episode: 9400, Avg timestpe : 478.9\n",
      "# of episode: 9420, Avg timestpe : 489.85\n",
      "# of episode: 9440, Avg timestpe : 467.5\n",
      "# of episode: 9460, Avg timestpe : 466.7\n",
      "# of episode: 9480, Avg timestpe : 471.1\n",
      "# of episode: 9500, Avg timestpe : 482.55\n",
      "# of episode: 9520, Avg timestpe : 459.2\n",
      "# of episode: 9540, Avg timestpe : 478.1\n",
      "# of episode: 9560, Avg timestpe : 467.5\n",
      "# of episode: 9580, Avg timestpe : 484.5\n",
      "# of episode: 9600, Avg timestpe : 474.7\n",
      "# of episode: 9620, Avg timestpe : 495.25\n",
      "# of episode: 9640, Avg timestpe : 490.0\n",
      "# of episode: 9660, Avg timestpe : 415.55\n",
      "# of episode: 9680, Avg timestpe : 428.6\n",
      "# of episode: 9700, Avg timestpe : 395.9\n",
      "# of episode: 9720, Avg timestpe : 442.75\n",
      "# of episode: 9740, Avg timestpe : 464.45\n",
      "# of episode: 9760, Avg timestpe : 466.65\n",
      "# of episode: 9780, Avg timestpe : 487.45\n",
      "# of episode: 9800, Avg timestpe : 485.45\n",
      "# of episode: 9820, Avg timestpe : 500.0\n",
      "# of episode: 9840, Avg timestpe : 485.7\n",
      "# of episode: 9860, Avg timestpe : 484.65\n",
      "# of episode: 9880, Avg timestpe : 478.6\n",
      "# of episode: 9900, Avg timestpe : 496.8\n",
      "# of episode: 9920, Avg timestpe : 462.1\n",
      "# of episode: 9940, Avg timestpe : 500.0\n",
      "# of episode: 9960, Avg timestpe : 496.65\n",
      "# of episode: 9980, Avg timestpe : 479.6\n"
     ]
    }
   ],
   "source": [
    "#하이퍼파라미터 변수들\n",
    "learning_rate = 0.0002\n",
    "gamma = 0.98\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.data = []\n",
    "        self.fc1 = nn.Linear(4, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=0)\n",
    "        return x\n",
    "        \n",
    "    def put_data(self, item):\n",
    "        self.data.append(item)\n",
    "\n",
    "    def train_net(self):\n",
    "        R = 0\n",
    "        self.optimizer.zero_grad()\n",
    "        for r, prob in self.data[::-1]:\n",
    "            R = r + gamma * R\n",
    "            loss = -torch.log(prob) * R\n",
    "            loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.data = []\n",
    "\n",
    "#reinforce 알고리즘 메인 함수\n",
    "def main():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    pi = Policy()\n",
    "    score = 0.0\n",
    "    print_interval = 20\n",
    "\n",
    "    for n_epi in range(10000):  \n",
    "        s, info = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:    \n",
    "            prob = pi(torch.from_numpy(s).float())\n",
    "            m = Categorical(prob)\n",
    "            a = m.sample()\n",
    "            s_prime, r, terminated, truncated, info = env.step(a.item())\n",
    "            done = terminated or truncated\n",
    "            pi.put_data((r,prob[a]))\n",
    "            s = s_prime\n",
    "            score += r\n",
    "            \n",
    "        pi.train_net()\n",
    "        \n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            print(\"# of episode: {}, Avg timestpe : {}\".format(n_epi, score/print_interval))\n",
    "            score = 0.0\n",
    "    env.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4566f4d2-9721-4487-8b04-f7084111c126",
   "metadata": {},
   "source": [
    "https://github.com/multicore-it/rl/blob/main/codes/cartpole_reinforce.ipynb\n",
    "위의 깃허브 링크에서 얻어온 Reinforce cartpole 예제 코드 실습하기에 아주 좋음!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77b81c26-8c22-4c27-817f-09f2f2850310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user1\\anaconda3\\envs\\masan\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_states (InputLayer)   [(None, 1, 4)]               0         []                            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 1, 12)                60        ['input_states[0][0]']        \n",
      "                                                                                                  \n",
      " input_action_matrixs (Inpu  [(None, 1, 2)]               0         []                            \n",
      " tLayer)                                                                                          \n",
      "                                                                                                  \n",
      " input_rewards (InputLayer)  [(None, 1, 1)]               0         []                            \n",
      "                                                                                                  \n",
      " output (Dense)              (None, 1, 2)                 26        ['dense[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 86 (344.00 Byte)\n",
      "Trainable params: 86 (344.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\user1\\anaconda3\\envs\\masan\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\user1\\anaconda3\\envs\\masan\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\user1\\anaconda3\\envs\\masan\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\user1\\AppData\\Local\\Temp\\ipykernel_31996\\2410009075.py\", line 39, in train_step\n        y_pred = self(states, training=True)\n    File \"C:\\Users\\user1\\anaconda3\\envs\\masan\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\user1\\anaconda3\\envs\\masan\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"my_model\" expects 3 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 1, 4) dtype=float32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 144\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    143\u001b[0m     agent \u001b[38;5;241m=\u001b[39m Agent()\n\u001b[1;32m--> 144\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 68\u001b[0m, in \u001b[0;36mAgent.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mmax_episode_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[0;32m     67\u001b[0m count, reward_tot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_memory(episode, state)\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_mini_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear_memory()\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m:\n",
      "Cell \u001b[1;32mIn[5], line 129\u001b[0m, in \u001b[0;36mAgent.train_mini_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    127\u001b[0m action_matrixs_t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_matrixs)\n\u001b[0;32m    128\u001b[0m action_probs_t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_probs)\n\u001b[1;32m--> 129\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mstates_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_matrixs_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscount_rewards_t\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43maction_probs_t\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs_cnt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\masan\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file350_hc7r.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 39\u001b[0m, in \u001b[0;36mAgent.MyModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     36\u001b[0m states, action_matrix, rewards \u001b[38;5;241m=\u001b[39m in_datas[\u001b[38;5;241m0\u001b[39m], in_datas[\u001b[38;5;241m1\u001b[39m], in_datas[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 39\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     action_probs \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39msum(action_matrix\u001b[38;5;241m*\u001b[39my_pred, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     41\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mK\u001b[38;5;241m.\u001b[39mlog(action_probs)\u001b[38;5;241m*\u001b[39mrewards\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\user1\\anaconda3\\envs\\masan\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\user1\\anaconda3\\envs\\masan\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\user1\\anaconda3\\envs\\masan\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\user1\\AppData\\Local\\Temp\\ipykernel_31996\\2410009075.py\", line 39, in train_step\n        y_pred = self(states, training=True)\n    File \"C:\\Users\\user1\\anaconda3\\envs\\masan\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\user1\\anaconda3\\envs\\masan\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"my_model\" expects 3 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 1, 4) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import gym\n",
    "import numpy as np\n",
    "import random as rand\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.value_size = 1\n",
    "\n",
    "        self.node_num = 12\n",
    "        self.learning_rate = 0.0005\n",
    "        self.epochs_cnt = 5\n",
    "        self.model = self.build_model()\n",
    "\n",
    "        self.discount_rate = 0.95\n",
    "        self.penalty = -10\n",
    "        self.episode_num = 500\n",
    "        self.moving_avg_size = 20\n",
    "\n",
    "        self.reward_list = []\n",
    "        self.count_list = []\n",
    "        self.moving_avg_list = []\n",
    "        self.states, self.action_matrixs, self.action_probs, self.rewards = [], [], [], []\n",
    "\n",
    "        self.DUMMY_ACTION_MATRIX, self.DUMMY_REWARD = np.zeros((1, 1, self.action_size)), np.zeros((1,1,self.value_size))\n",
    "\n",
    "    class MyModel(tf.keras.Model):\n",
    "        def train_step(self, data):\n",
    "            in_datas, out_actions = data\n",
    "            states, action_matrix, rewards = in_datas[0], in_datas[1], in_datas[2]\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = self(states, training=True)\n",
    "                action_probs = K.sum(action_matrix*y_pred, axis=-1)\n",
    "                loss = -K.log(action_probs)*rewards\n",
    "\n",
    "            trainable_vars = self.trainable_variables\n",
    "            gradients = tape.gradient(loss, trainable_vars)\n",
    "            self.optimizers.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "    def build_model(self):\n",
    "        input_states = Input(shape=(1,self.state_size), name='input_states')\n",
    "        input_action_matrixs = Input(shape=(1, self.action_size), name='input_action_matrixs')\n",
    "        input_rewards = Input(shape=(1,self.value_size), name='input_rewards')\n",
    "\n",
    "        x = (input_states)\n",
    "        x = Dense(self.node_num, activation='tanh')(x)\n",
    "        out_actions = Dense(self.action_size, activation='softmax', name='output')(x)\n",
    "\n",
    "        model = self.MyModel(inputs=[input_states, input_action_matrixs, input_rewards], outputs=out_actions)\n",
    "        model.compile(optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(self.episode_num):\n",
    "            state = self.env.reset()\n",
    "            state = state[0]\n",
    "            self.env.max_episode_steps = 500\n",
    "\n",
    "            count, reward_tot = self.make_memory(episode, state)\n",
    "            self.train_mini_batch()\n",
    "            self.clear_memory()\n",
    "\n",
    "            if count < 500:\n",
    "                reward_tot = reward_tot - self.penalty\n",
    "\n",
    "            self.reward_list.append(reward_tot)\n",
    "            self.count_list.append(count)\n",
    "            self.moving_avg_list.append(self.moving_avg(self.count_list, self.moving_avg_size))\n",
    "\n",
    "            if(episode % 10 == 0):\n",
    "                print(\"episode:{}, moving_avg:{}, rewards_avg:{}\".format(episode, self.moving_avg_list[-1], np.mean(self.reward_list)))\n",
    "\n",
    "        self.save_model()\n",
    "\n",
    "    def make_memory(self, episode, state):\n",
    "        reward_tot = 0\n",
    "        count = 0\n",
    "        reward = np.zeros(self.value_size)\n",
    "        action_matrix = np.zeros(self.action_size)\n",
    "        done = False\n",
    "        while not done:\n",
    "            count += 1\n",
    "            state_t = np.reshape(state, [1, 1, self.state_size])\n",
    "            action_matrix_t = np.reshape(action_matrix, [1, 1, self.action_size])\n",
    "\n",
    "            action_prob = self.model.predict([state_t, self.DUMMY_ACTION_MATRIX, self.DUMMY_REWARD])\n",
    "            action = np.random.choice(self.action_size, 1, p=action_prob[0][0])[0]\n",
    "            action_matrix = np.zeros(self.action_size)\n",
    "            action_matrix[action] = 1\n",
    "            state_next, reward, done, none, none2 = self.env.step(action)\n",
    "\n",
    "            if count < 500 and done:\n",
    "                reward = self.penalty\n",
    "\n",
    "            self.states.append(np.reshape(state_t, [1, self.state_size]))\n",
    "            self.action_matrixs.append(np.reshape(action_matrix, [1, self.action_size]))\n",
    "            self.action_probs.append(np.reshape(action_prob, [1, self.action_size]))\n",
    "            self.rewards.append(reward)\n",
    "            reward_tot += reward\n",
    "            state = state_next\n",
    "        return count, reward_tot\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states, self.action_matrixs, self.action_probs, self.rewards = [], [], [], []\n",
    "\n",
    "    def make_discount_rewards(self, rewards):\n",
    "        discounted_rewards = np.zeros(np.array(rewards).shape)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            running_add = running_add * self.discount_rate + rewards[t]\n",
    "            discounted_rewards[t] = running_add\n",
    "\n",
    "        return discounted_rewards\n",
    "\n",
    "    def train_mini_batch(self):\n",
    "        discount_rewards = np.array(self.make_discount_rewards(self.rewards))\n",
    "        discount_rewards_t = np.reshape(discount_rewards, [len(discount_rewards), 1, 1])\n",
    "        states_t = np.array(self.states)\n",
    "        action_matrixs_t = np.array(self.action_matrixs)\n",
    "        action_probs_t = np.array(self.action_probs)\n",
    "        self.model.fit(x=[states_t, action_matrixs_t, discount_rewards_t], y=[action_probs_t], epochs=self.epochs_cnt, verbose=0)\n",
    "\n",
    "    def moving_avg(self, data, size=10):\n",
    "        if len(data) > size:\n",
    "            c = np.array(data[len(data)-size:len(data)])\n",
    "        else:\n",
    "            c= np.array(data)\n",
    "        return np.mean(c)\n",
    "\n",
    "    def save_model(self):\n",
    "        self.model.save(\"jaedeok_reinforce_example\")\n",
    "        print(\"It's finish!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent = Agent()\n",
    "    agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b687f88c-3a58-4967-8ac7-d55513307422",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masan",
   "language": "python",
   "name": "masan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
